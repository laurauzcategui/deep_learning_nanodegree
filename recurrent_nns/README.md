# Recurrent Neural Networks Module

On this module I was able to grasp concepts related to how Recurrent neural networks works and how they are used, also it was possible to implement/use Word Embeddings, specifically to get to know a bit deeper the Word2vec model. 

Recurrent neural networks (RNNs), are associated with Text processing and Text generation among other tasks/applications. The difference with other network types is that they incorporate memory/state into its architecture. 

Checkout the Lessons:

### [Lesson 3](./l3_implementing_rnns_lstms): Implementing RNNs - LSTMs 

- Learn about RNNs & LSTMs
- Implement a character level RNN on the famous piece Anna Karenina. 
- Implement time series predictions using RNNs

### [Lesson 5](./l5_embeddings_word2vec): Embeddings & Word2vec

- Learn about word embeddings, how they work
- Learn about Word2Vec, mappings of Word embeddings to give semantice meaning.

### [Lesson 6](./l6_sentiment_rnns): Sentiment RNNs

- Learn how to implement a model that use embedding and LSTMs to read text and to do sentiment prediction on Movie Reviews. 

### [Project](./project-tv-script-generation): Generation of TV scripts

- The idea of the project was to take text composed of multiple sentences for the famous TV show Seinfield 

 By creating a tokenizer of the text into numbers and generation of batches and after passing it to a RNN model we will be able to generate TV scripts like seinfield. 

 This was a fun project, nevertheless I feel It could be improved. 

 ### [Lesson 8](./l8_attention)

- Last lesson of the model was focused on attention and how this kind of models are used as a combination of encoders/decoders, feedforward / rnns / transformer architectures that are being used on machine translation, caption generation, chatbots among others. 


## Resources

- [Vanishing Gradient Problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)
- [Understanding Geometric Series](https://socratic.org/algebra/exponents-and-exponential-functions/geometric-sequences-and-exponential-functions)
- [LSTM's paper](http://www.bioinf.jku.at/publications/older/2604.pdf)
- [Session based recommendations with RNNs](https://arxiv.org/pdf/1511.06939.pdf)
- [Building an efficient neural language model over a billion words](https://engineering.fb.com/ml-applications/building-an-efficient-neural-language-model-over-a-billion-words/) 
- [On the difficulty of training Recurrent Neural Networks](https://arxiv.org/pdf/1211.5063.pdf)
- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [Exploring LSTMs](http://blog.echen.me/2017/05/30/exploring-lstms/)
- [CS231n Lecture 10 - Recurrent Neural Networks, Image Captioning, LSTM](https://www.youtube.com/watch?v=iX5V1WpxxkY)
- [Learning Long Term Dependencies with RNN](http://www.cs.toronto.edu/~guerzhoy/321/lec/W09/rnn_gated.pdf)

